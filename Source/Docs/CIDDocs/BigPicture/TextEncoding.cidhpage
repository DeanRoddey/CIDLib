<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE HelpPage PUBLIC "urn:charmedquark.com:CIDLib-Documentation.DTD" "CIDLibDocs.DTD">

<HelpPage>
    <Title>Text Encoding</Title>

    <HelpText>

        <P>See the Text/Strings section to the left for general information on text in CIDLib. Here I'll get a bit more into text transcoding, which is the process of converting text from one encoding to another.</P>

        <P>Internally, the text is always in the native wide character format. But when you need to read in or write out text, or receive text from/send text to some server, it will almost never be in the wide character format (because that's both endian specific and inefficient, for reasons discussed in the Text/Strings section.)</P>

        <P>These days, the most commonly seen external encoding is UTF-8, because it can represent all valid Unicode code points, and it stores them with the least possible number of bytes. But you will still run into various ISO formats (such as 8859-1, 8859-2, and so forth) and of course simple ASCII text, or in the Windows world the common 1252 code page used by Windows for a long time.</P>

        <P>CIDLib has an extensible transcoding system, based on the <ClassRef Ref="TTextConverter"/> base class in the CIDLib facility. This defines the interface required to convert code from an external encoding to the wide character format, and vice versa. Because US-ASCII, UTF-8, UTF-16, and UTF-32 are so common, derivatives for those encodings are also provided in the CIDLib facility so that they are always available. </P>

        <P>Other encodings are provided by the <FacRef Ref="CIDEncode"/> facility, so include that if you need to support other encodings. CIDEncode doesn't support a huge range of encodings, since it's getting more and more common for UTF-8 to be used for storage and transmission of text. But, it supports a good number of encodings you may need, and others can be added if necessary.</P>

        <P>You don't directly create text converter objects for these CIDEncode based converters, instead the CIDEncoding facility object, facCIDEncode(), acts as a factory for creating them. You just provide the encoding name (and it supports many common variations of each encodings name) and it will generate an object for you (if supported) and return a pointer to it. You are responsible for cleaning it up when done with it. You can also ack to get it already encapsulated in a counted pointer if that's more convenient for you.</P>

        <P>The facility object also provides some other very useful functionality, such as probing a memory buffer and trying to determine if it is obviously one of a set of recognizable encodings. For instance, stored UTF-x code will often start with a 'byte order mark' to indicate what form of UTF it is, and that can be recognized. Currently those are the only ones that stand a reasonable chance of being recognized in an arbitrary block of encoded text. Otherwise it would require that it start with some known text, such as is the case in XML.</P>

        <Note>Converting from the internal wide character format to the external encoding is often referred to as 'extenralization', and vice versa going from the external encoding to the internal wide character format is often referred to as 'internalization', and you will see those terms used in this documentation.</Note>

        <SecTitle>Thread Safety</SecTitle>

        <P>On the whole, text converters work purely on the data passed to them and have no state of their own, hence they can be used by multiple threads simultaneously. But don't just assume that, check the documentation for the specific converter class. On the other hand, that also means that they are so light weight that having one of them for each producer/consumder of encoded text isn't much of a burden either, and you don't even have to worry about it.</P>

        <P>See the discussion on Thread Safety in the Philosophy section to the left for more discussion of this issue.</P>

        <SecTitle>Chunked Processing</SecTitle>

        <P>One reason for the above thread safe design is because they can't assume that they are just being fed text from some single, contiguous source, or even sequential chunks from such. They may get chunks of text that are unrelated and cannot assume any context across such calls. They must treat each chunk individually.</P>

        <P>That in turn means that you cannot assume, when internalizing from an external encoding, that all the bytes you pass will be converted. If that chunk happens to end in the middle of a code point, only the bytes up through the last fully represented code point can be processed. So you always have to be prepared to 'carry over' text from one chunk to the next one if you don't pass an entire valid chunk of content at once. They always tell you how many input bytes they actually processed.</P>

        <SecTitle>Invalid/Unrepresentable Text</SecTitle>

        <P>There is always the chance that, when internalizing data in from the encoded format to the wide character format, that there can be bad data. It might be corrupted or badly encoded or not in the encoding that the program was told to assume. When going the other way, it's always possible that you have wide characters that just can't be represented in the target encoding. You have to tell the text encoder how to deal with such things, using the tCIDLib::ETCvtActions enum, which has these options:</P>

        <List>
            <LItem Ref="Replacement Char">Use a configured replacement character for characters that cannot be represented</LItem>
            <LItem Ref="Stop and Throw">Returns any text that has been processed so far, and put off throwing an error until the next round.</LItem>
            <LItem Ref="Throw">Discard any text that has been processed so far and just throw immediately.</LItem>
        </List>

        <P>The first option is used for externalization (wide character to encoding) when the wide character cannot be represented in the target encoding. So you might choose to replace such characters with a space or some such. For internalization, it is mostly only useful for single byte encodings, where an invalid character isn't going to throw off all of the data from there forward in most cases. I.e. in a variable byte encoding, a single bad character is generally going to scramble everything from there forward and even trying to decode it is probably not worth it.</P>

        <P>So, in the case where the particular converter knows it's not worth trying to continue internalization it will just throw an error, even if the replacement character option is set.</P>

        <P>The other two treat the inability to represent a character in the target encoding or any bad characters in the data being internalized. You have two options. You can either just throw immediately, or you can return any text successfully processed so far. On the next round the error will be seen again and the throw will happen that time because zero bytes will be processed.</P>

        <P>You might thing that it's not worth doing the Stop and Throw scenario, but it's actually very important in specific cases. For instance, if you are internalizaing  an XML or JSON file, you want to tell the user where in the input the data went bad. But you can only do that if you parse all of the actually good data first, so that you know the line/column information where the parsing failed.</P>

    </HelpText>

</HelpPage>
